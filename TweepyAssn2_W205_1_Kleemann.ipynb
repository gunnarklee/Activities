{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment 2 \n",
    "#Gunnar Kleemann               6/26/15\n",
    "#W205_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set path permissions and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd /Users/gunnarkleemann/Dropbox/coursework/BerkeleyDataSciences/BerkeleyCODE/205code/DataStoreRetrieval/Asn2Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import urllib\n",
    "#import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = XX\"H1MW\"\n",
    "consumer_secret = XX\"PDlIhV\"\n",
    "\n",
    "access_token = XX\"zbUT6\"\n",
    "access_token_secret = XX\"m3hY\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now acquire the tweets\n",
    "* looking back into the historical tweets for a week using **Tweepy's Cursor** \n",
    "* I am writing each tweet **line by line into a CSV file** so that they can be easily stored and analyzed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now save output - using the manual method\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def TwpyCursByTime(since, until, q):\n",
    "    count = 0\n",
    "    countLim = 2500 # failed after 2700 tweets last time\n",
    "    if not os.path.exists('TweetFldr'): \n",
    "       os.makedirs('TweetFldr')\n",
    "\n",
    "    csvFile = open('TweetFldr/out'+since+'_'+until+'_'+q+'.csv', 'a')\n",
    "    #Use csv Writer\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    # generate content writing line-wise\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                        q=q,\n",
    "                        since=since, \n",
    "                        until=until,\n",
    "                        lang=\"en\").items():\n",
    "\n",
    "        #Write a row to the csv file/ I use encode utf-8\n",
    "        csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "        #print tweet.created_at,tweet.text # print tweet.created_at, tweet.text\n",
    "        count+=1\n",
    "        if count==countLim:\n",
    "            return\n",
    "    csvFile.close()\n",
    "    next\n",
    "\n",
    "#since=\"2015-06-18\"\n",
    "#until=\"2015-06-19\"\n",
    "#q=urllib.quote_plus(\"#NBAFinals2015 AND #Warriors\")\n",
    "#TwpyCursByTime(since,until,q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a list of tuples of arguments you want to pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#since=\"2015-06-12\"\n",
    "#until=\"2015-06-13\"\n",
    "\n",
    "q=urllib.quote_plus(\"#Warriors NOT #NBAFinals2015\")\n",
    "q1=urllib.quote_plus(\"#NBAFinals2015 NOT #Warriors\")\n",
    "q2=urllib.quote_plus(\"#NBAFinals2015 AND #Warriors\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TagTimeInt=[(\"2015-06-12\",\"2015-06-13\",q),\n",
    "            (\"2015-06-12\",\"2015-06-13\",q1),\n",
    "            (\"2015-06-12\",\"2015-06-13\",q2),\n",
    "            (\"2015-06-13\",\"2015-06-14\",q),\n",
    "            (\"2015-06-13\",\"2015-06-14\",q1),\n",
    "            (\"2015-06-13\",\"2015-06-14\",q2),\n",
    "            (\"2015-06-14\",\"2015-06-15\",q),\n",
    "            (\"2015-06-14\",\"2015-06-15\",q1),\n",
    "            (\"2015-06-14\",\"2015-06-15\",q2),\n",
    "            (\"2015-06-15\",\"2015-06-16\",q),\n",
    "            (\"2015-06-15\",\"2015-06-16\",q1),\n",
    "            (\"2015-06-15\",\"2015-06-16\",q2),\n",
    "            (\"2015-06-16\",\"2015-06-17\",q),\n",
    "            (\"2015-06-16\",\"2015-06-17\",q1),\n",
    "            (\"2015-06-16\",\"2015-06-17\",q2),\n",
    "            (\"2015-06-17\",\"2015-06-18\",q),\n",
    "            (\"2015-06-17\",\"2015-06-18\",q1),\n",
    "            (\"2015-06-17\",\"2015-06-18\",q2),\n",
    "            (\"2015-06-18\",\"2015-06-19\",q),\n",
    "            (\"2015-06-18\",\"2015-06-19\",q1),\n",
    "            (\"2015-06-18\",\"2015-06-19\",q2)\n",
    "           ]\n",
    "\n",
    "print(TagTimeInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I explicitly pause the loop to prevent from overloading the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "for n in range(len(TagTimeInt)):\n",
    "    TwpyCursByTime(TagTimeInt[n][0], TagTimeInt[n][1], TagTimeInt[n][2])\n",
    "    #Show the rate Limits\n",
    "    print api.rate_limit_status()\n",
    "    # pause before attempting next acquire\n",
    "    print'2min Pause before exceding limit'\n",
    "    print 'Next up '+ TagTimeInt[n][0] + TagTimeInt[n][1] + TagTimeInt[n][2]\n",
    "    str(datetime.now())\n",
    "    time.sleep(120)   #getting very few tweetes pause less(120sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in this step I retrieve the tweets, process the text and plot the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/gunnarkleemann/Dropbox/coursework/BerkeleyDataSciences/BerkeleyCODE/205code/DataStoreRetrieval/Asn2Code') \n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# go to the folder hoding data subfolders\n",
    "os.chdir('TweetFldr')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a list of files from each query\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.chdir(\"/Users/gunnarkleemann/Dropbox/coursework/BerkeleyDataSciences/BerkeleyCODE/205code/DataStoreRetrieval/Asn2Code/TweetFldr/\")\n",
    "!pwd\n",
    "SubFolder= (\"tweepy_set1CSV\")\n",
    "\n",
    "FileLsNoWar=glob.glob(SubFolder+\"/*NOT+%23Warriors.csv\")\n",
    "FileLsNoFinal=glob.glob(SubFolder+\"/*NOT+%23NBAFinals2015.csv\")\n",
    "FileLsBOTH=glob.glob(SubFolder+\"/*ND+%23Warriors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the paths\n",
    "FileLsBOTH[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FileLsNoFinal[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FileLsNoWar[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employ a function that takes in lists of files and combines them into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ListToDf(allFiles):\n",
    "    #frame = pd.DataFrame(names=['time', 'tweettxt'], index_col=None)\n",
    "    frame = pd.read_csv(allFiles[0], names=['time', 'tweettxt'], index_col=None)\n",
    "    #frame.columns= ['dateTime','tweetTxt']\n",
    "    for n in range(len(allFiles)):\n",
    "        df= pd.read_csv(allFiles[n], names=['time', 'tweettxt'], index_col=None)\n",
    "        frame=df.append(frame, ignore_index=True)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FileLsNoWar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NoWar=ListToDf(FileLsNoWar)\n",
    "NoFinal=ListToDf(FileLsNoFinal)\n",
    "Both=ListToDf(FileLsBOTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Both.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all of the tweets into one big text string (\"Blob\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BlobTheData(data):\n",
    "    TweetBlob=\"\"\n",
    "    for n in range(len(data)):\n",
    "         TweetBlob=TweetBlob + str(data[n])\n",
    "    return TweetBlob\n",
    "\n",
    "\n",
    "NoWarBlob=BlobTheData(NoWar.tweettxt)\n",
    "NoFinalBlob=BlobTheData(NoFinal.tweettxt)\n",
    "BothBlob=BlobTheData(Both.tweettxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now tokenize, filter and count the words for each Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NoWarBlob = \"# coding=utf-8 \\n\" + NoWarBlob\n",
    "#print NoWarBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import matplotlib\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# tokenize everything so we dont break up words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "TokNoWar=tokenizer.tokenize(NoWarBlob)\n",
    "TokNoFinal=tokenizer.tokenize(NoFinalBlob)\n",
    "TokBoth=tokenizer.tokenize(BothBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punctuation with string comprehension\n",
    "TokNoWar = [w.lower() for w in TokNoWar if w.isalpha()]\n",
    "TokNoFinal = [w.lower() for w in TokNoFinal if w.isalpha()]\n",
    "TokBoth= [w.lower() for w in TokBoth if w.isalpha()]\n",
    "#print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out stop words and other contaminants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "# add custom filter words \n",
    "OtherFilter = [u\"http\", u\"co\"]\n",
    "stopWords=OtherFilter+stopWords\n",
    "#print stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove stop words \n",
    "TokNoWar = [e.lower() for e in TokNoWar  if not e.lower() in stopWords]\n",
    "TokNoFinal = [e.lower() for e in TokNoFinal if not e.lower() in stopWords]\n",
    "TokBoth = [e.lower() for e in TokBoth if not e.lower() in stopWords]\n",
    "#TokBoth\n",
    "#filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a distribution\n",
    "\n",
    "DS1=nltk.FreqDist(TokNoWar)\n",
    "DS1.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS2=nltk.FreqDist(TokNoFinal)\n",
    "DS2.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS3=nltk.FreqDist(TokBoth)\n",
    "DS3.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#REDO_ Looking at my results I realize that all of them contain both tags and the \"Warriors NOT NBAfinals2015\" is == \"NBAfinals2015 NOT warriors\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to address this problem I am going to use some older files where I searched on Just Warriors and just NBAfinals 2015. These are not a full week but it is all I have to run at this point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use awk to remove lines that incedentally include #NBAfinals2015 in the #Warriors files and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/gunnarkleemann/Dropbox/coursework/BerkeleyDataSciences/BerkeleyCODE/205code/DataStoreRetrieval/Asn2Code/OldStuff/tweepy_firsttrycsv')\n",
    "!pwd\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!awk '!/pattern/' file > temp && mv temp file\n",
    "\n",
    "!awk '!/#warriors/' \"out2015-06-12_2015-06-13_%23NBAFinals2015.csv\" > \"out2015-06-12NBAFIN.csv\" \n",
    "\n",
    "\n",
    "!awk '!/#warriors/' \"out2015-06-14_2015-06-15_%23NBAFinals2015.csv\" > \"out2015-06-14NBAFIN.csv\"\n",
    "\n",
    "!awk '!/#warriors/' \"out2015-06-13_2015-06-14_%23NBAFinals2015.csv\" > \"out2015-06-13NBAFIN.csv\" \n",
    "\n",
    "!awk '!/#NBAFinals2015/' \"out2015-06-12_2015-06-13_%23Warriors.csv\" > \"out2015-06-12Warriors.csv\"     \n",
    "     \n",
    "!awk '!/#NBAFinals2015/' \"out2015-06-13_2015-06-14_%23Warriors.csv\" > \"out2015-06-13Warriors.csv\"     \n",
    "      \n",
    "!awk '!/#NBAFinals2015/' \"out2015-06-14_2015-06-15_%23Warriors.csv\" > \"out2015-06-14Warriors.csv\"     \n",
    "     \n",
    "!awk '!/#NBAFinals2015/' \"out2015-06-15_2015-06-16_%23Warriors.csv\" > \"out2015-06-15Warriors.csv\"     \n",
    "            \n",
    "# check the differences for one pre/post cleaning pair of files to be sure that we are removing lines with both tags\n",
    "!diff \"out2015-06-12_2015-06-13_%23NBAFinals2015.csv\" \"out2015-06-12NBAFIN.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the cleaned files to generate a REDO of the #Warriors only and NBAfinals2015 only sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a list of files from each query\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#I moved the new files into the CleanFiles subfolder\n",
    "SubFolder= (\"CleanFiles\")\n",
    "\n",
    "FileLsNoWar=glob.glob(SubFolder+\"/*NBAFIN.csv\")\n",
    "FileLsNoFinal=glob.glob(SubFolder+\"/*Warriors.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Employ a function that takes in lists of files and combines them into a single dataframe\n",
    "NoWar=ListToDf(FileLsNoWar)\n",
    "NoFinal=ListToDf(FileLsNoFinal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Combine all of the tweets into one big text string (\"Blob\")\n",
    "def BlobTheData(data):\n",
    "    TweetBlob=\"\"\n",
    "    for n in range(len(data)):\n",
    "         TweetBlob=TweetBlob + str(data[n])\n",
    "    return TweetBlob\n",
    "\n",
    "\n",
    "NoWarBlob=BlobTheData(NoWar.tweettxt)\n",
    "NoFinalBlob=BlobTheData(NoFinal.tweettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now tokenize, filter and count the words for each Blob\n",
    "\n",
    "# tokenize everything so we dont break up words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "TokNoWar=tokenizer.tokenize(NoWarBlob)\n",
    "TokNoFinal=tokenizer.tokenize(NoFinalBlob)\n",
    "\n",
    "# remove punctuation with string comprehension\n",
    "TokNoWar = [w.lower() for w in TokNoWar if w.isalpha()]\n",
    "TokNoFinal = [w.lower() for w in TokNoFinal if w.isalpha()]\n",
    "\n",
    "#Filter out stop words and other contaminants\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "# add custom filter words \n",
    "OtherFilter = [u\"http\", u\"co\"]\n",
    "stopWords=OtherFilter+stopWords\n",
    "\n",
    "# remove stop words \n",
    "TokNoWar = [e.lower() for e in TokNoWar  if not e.lower() in stopWords]\n",
    "TokNoFinal = [e.lower() for e in TokNoFinal if not e.lower() in stopWords]\n",
    "TokBoth = [e.lower() for e in TokBoth if not e.lower() in stopWords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a distribution\n",
    "\n",
    "DS1=nltk.FreqDist(TokNoWar)\n",
    "DS1.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS2=nltk.FreqDist(TokNoFinal)\n",
    "DS2.plot(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
